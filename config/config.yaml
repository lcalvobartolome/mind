################################
#     LOGGING CONFIGURATION    #
################################
logger:
  dir_logger: data/logs
  console_log: True
  file_log: True
  log_level: INFO
  logger_name: mind
  N_log_keep: 5 #maximum number of log files to keep

################################
#      MIND CONFIGURATION      #
################################
mind:
  top_k: 10
  batch_size: 32
  min_clusters: 8
  do_weighting: True
  cannot_answer_dft: I cannot answer the question given the context.
  cannot_answer_personal: I cannot answer the question since the context only contains personal opinions.
  prompts:
    question_generation: src/mind/pipeline/prompts/question_generation.txt
    subquery_generation: src/mind/pipeline/prompts/query_generation.txt
    answer_generation: src/mind/pipeline/prompts/question_answering.txt
    contradiction_checking: src/mind/pipeline/prompts/discrepancy_detection.txt
    relevance_checking: src/mind/pipeline/prompts/relevance_checking.txt
  embedding_models:
    multilingual: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
    monolingual: 
      en: sentence-transformers/all-MiniLM-L6-v2
  nli_model_name: potsawee/deberta-v3-large-mnli

################################
#   PROMPTING CONFIGURATION    #
################################
llm:
  parameters:
    temperature: 0
    top_p: 0.1
    frequency_penalty: 0.0
    random_seed: 1234
    seed: 1234
  gpt:
    available_models:
      {
        "gpt-4o-2024-08-06",
        "gpt-4o-mini-2024-07-18",
        "chatgpt-4o-latest",
        "gpt-4-turbo",
        "gpt-4-turbo-2024-04-09",
        "gpt-4",
        "gpt-3.5-turbo",
        "gpt-4o-mini",
        "gpt-4o",
        "gpt-4-32k",
        "gpt-4-0125-preview",
        "gpt-4-1106-preview",
        "gpt-4-vision-preview",
        "gpt-3.5-turbo-0125",
        "gpt-3.5-turbo-instruct",
        "gpt-3.5-turbo-1106",
        "gpt-3.5-turbo-0613",
        "gpt-3.5-turbo-16k-0613",
        "gpt-3.5-turbo-0301",
      }
    path_api_key: .env
  ollama:
    available_models: {
      "qwen2.5:72b",
      "llama3.2",
      "llama3.1:8b-instruct-q8_0",
      "qwen:32b",
      "llama3.3:70b",
      "qwen2.5:7b-instruct",
      "qwen3:32b"
    }
    host: http://kumo01.tsc.uc3m.es:11434
  vllm:
    available_models: {
      "Qwen/Qwen3-8B",
      "Qwen/Qwen3-0.6B",
      "meta-llama/Meta-Llama-3-8B-Instruct"
    }
    host: http://kumo01.tsc.uc3m.es:6000/v1
  llama_cpp:
    host: http://kumo01:11435/v1/chat/completions